# multimodal-skin-lesion-efficientnet
A multi-modal deep learning framework for skin lesion classification that combines dermoscopic images with clinical metadata using an EfficientNet-based CNN and an MLP branch. Evaluated on the HAM10000 dataset with class imbalance handling.

This model represents one of the architectures evaluated in our study and introduces a multimodal learning strategy that jointly leverages dermoscopic images and structured clinical metadata from the HAM10000 dataset. Unlike earlier models that relied exclusively on image-based features—such as custom CNNs, DenseNet121, EfficientNetB1, and other single-input transfer-learning approaches—this model explicitly integrates patient-level information, including age, sex, and lesion localization, alongside deep visual representations. The motivation behind this design is to more closely mimic real-world clinical decision-making, where dermatologists consider both visual appearance and contextual patient data.

The dataset preprocessing pipeline was extended to handle both modalities. Missing age values were imputed using the mean age, categorical variables such as sex and anatomical localization were one-hot encoded, and the age feature was standardized to ensure numerical stability. These metadata features were then aligned with their corresponding dermoscopic images. The dataset was split using stratified sampling to preserve class distributions, resulting in the same train–validation proportions as in previous experiments, thereby enabling a fair comparison with earlier models.

The architecture consists of two parallel branches. The image branch uses EfficientNetB0 pretrained on ImageNet as a frozen feature extractor, followed by global average pooling and dropout for regularization. Compared to heavier backbones used in some earlier experiments, EfficientNetB0 offers a more compact and computationally efficient representation while retaining strong feature extraction capabilities. The metadata branch is implemented as a lightweight multilayer perceptron with two fully connected layers, designed to capture nonlinear interactions among clinical variables without overfitting. The outputs of these two branches are concatenated and passed through additional dense layers to learn joint representations before final classification.

As with previous models, severe class imbalance in HAM10000 was addressed through the use of class-weighted loss. This strategy again proved essential for stabilizing training and preventing the model from collapsing toward the majority class. Training was conducted using early stopping based on validation loss to reduce overfitting and ensure robust convergence.

The multimodal model achieved a final validation accuracy of 72.39%, which represents a clear improvement over earlier image-only models, including the EfficientNetB1 feature-extraction baseline and is competitive with the fine-tuned EfficientNetB1 model. Notably, this improvement was obtained without fine-tuning the image backbone, indicating that the inclusion of metadata provides complementary information that enhances discriminative performance. Compared to earlier custom CNN architectures and single-stream transfer-learning models, this approach demonstrates superior generalization and a more balanced performance across classes.

From a methodological perspective, this model highlights an important distinction from previous experiments: performance gains are achieved not solely through architectural scaling or deeper fine-tuning, but through information fusion across modalities. While challenges related to minority-class recall remain—consistent with observations from earlier models—the multimodal design offers a promising direction for further improvements, such as metadata-aware attention mechanisms or fine-tuning the image backbone jointly with clinical features.

Overall, this model establishes multimodal learning as one of the most effective strategies explored in this project. In comparison to earlier image-only approaches, it achieves higher accuracy with relatively modest model complexity and provides a more clinically meaningful framework for skin lesion classification.

The HAM10000 dataset used in this study is publicly available at:
(https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000)
